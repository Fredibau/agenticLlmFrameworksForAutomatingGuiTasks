{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4264a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Task ID Definitions ---\n",
    "# These dictionaries define the IDs for Easy, Medium, and Hard tasks.\n",
    "EASY_TASK_IDS = [\n",
    "    \"bb5e4c0d-f964-439c-97b6-bdb9747de3f4\",\n",
    "    \"030eeff7-b492-4218-b312-701ec99ee0cc\",\n",
    "    \"2ad9387a-65d8-4e33-ad5b-7580065a27ca\",\n",
    "    \"7a5a7856-f1b6-42a4-ade9-1ca81ca0f263\",\n",
    "    \"9656a811-9b5b-4ddf-99c7-5117bcef0626\",\n",
    "]\n",
    "\n",
    "MEDIUM_TASK_IDS = [\n",
    "    \"1704f00f-79e6-43a7-961b-cedd3724d5fd\",\n",
    "    \"6c4c23a1-42a4-43cc-9db1-2f86ff3738cc\",\n",
    "    \"cabb3bae-cccb-41bd-9f5d-0f3a9fecd825\",\n",
    "]\n",
    "\n",
    "HARD_TASK_IDS = [\n",
    "    \"121ba48f-9e17-48ce-9bc6-a4fb17a7ebba\",\n",
    "    \"6766f2b8-8a72-417f-a9e5-56fcaa735837\",\n",
    "]\n",
    "ALL_TASK_DIFFICULTIES = {\n",
    "    \"Easy\": EASY_TASK_IDS,\n",
    "    \"Medium\": MEDIUM_TASK_IDS,\n",
    "    \"Hard\": HARD_TASK_IDS,\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def calculate_average_results(base_path, category_name=\"\"):\n",
    "    \"\"\"\n",
    "    Goes through all ID directories, extracts values from result.txt,\n",
    "    and calculates the average.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base path leading to the ID directories.\n",
    "        category_name (str): An optional name for the location being processed, for clearer output.\n",
    "\n",
    "    Returns:\n",
    "        float: The average of the results, or None if no results are found.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Error: Base path for {category_name or 'unnamed location'} ('{base_path}') does not exist.\")\n",
    "        return None\n",
    "\n",
    "    for item in os.listdir(base_path):\n",
    "        item_path = os.path.join(base_path, item)\n",
    "\n",
    "        if os.path.isdir(item_path):\n",
    "            result_file_path = os.path.join(item_path, \"result.txt\")\n",
    "\n",
    "            if os.path.exists(result_file_path):\n",
    "                with open(result_file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                    results.append(value)\n",
    "\n",
    "\n",
    "    if not results:\n",
    "        print(f\"No valid result.txt files found in '{category_name or base_path}' to calculate an average.\")\n",
    "        return None\n",
    "    else:\n",
    "        average = sum(results) / len(results)\n",
    "        return average\n",
    "\n",
    "def analyze_task_success(base_path, task_ids_by_difficulty):\n",
    "    \"\"\"\n",
    "    Analyzes the success count for tasks categorized by difficulty.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base path for the current run/system (e.g., .../first/pyautogui/screenshot/ui-tars/chrome).\n",
    "        task_ids_by_difficulty (dict): A dictionary mapping difficulty levels (e.g., \"Easy\")\n",
    "                                       to a list of task IDs belonging to that difficulty.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are difficulty levels and values are the\n",
    "              count of successful tasks for that difficulty in the given base_path.\n",
    "              Returns counts of zeros if base_path does not exist.\n",
    "    \"\"\"\n",
    "    success_counts = {difficulty: 0 for difficulty in task_ids_by_difficulty.keys()}\n",
    "\n",
    "    if not os.path.exists(base_path):\n",
    "        return success_counts \n",
    "\n",
    "    for difficulty, task_ids in task_ids_by_difficulty.items():\n",
    "        successful_count = 0\n",
    "        for task_id in task_ids:\n",
    "            # Construct the path to the result.txt for this specific task ID\n",
    "            result_file_path = os.path.join(base_path, task_id, \"result.txt\")\n",
    "            \n",
    "            if os.path.exists(result_file_path):\n",
    "                with open(result_file_path, 'r') as f:\n",
    "                    value = float(f.read().strip())\n",
    "                    if value == 1.0: # A task is successful if its result.txt contains 1.0\n",
    "                        successful_count += 1\n",
    "\n",
    "        success_counts[difficulty] = successful_count\n",
    "    return success_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561ae81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating Averages and Analyzing Task Success Across All Runs and Systems ---\n",
      "\n",
      "--- Processing Run: First ---\n",
      "  Overall Average for First - Baseline System: 0.10\n",
      "  Successful Tasks for First - Baseline System:\n",
      "    Easy: 1/5 successful\n",
      "    Medium: 0/3 successful\n",
      "    Hard: 0/2 successful\n",
      "  Overall Average for First - Hierarchical Manager-Worker System: 0.30\n",
      "  Successful Tasks for First - Hierarchical Manager-Worker System:\n",
      "    Easy: 3/5 successful\n",
      "    Medium: 0/3 successful\n",
      "    Hard: 0/2 successful\n",
      "\n",
      "--- Processing Run: Second ---\n",
      "  Overall Average for Second - Baseline System: 0.20\n",
      "  Successful Tasks for Second - Baseline System:\n",
      "    Easy: 1/5 successful\n",
      "    Medium: 0/3 successful\n",
      "    Hard: 1/2 successful\n",
      "  Overall Average for Second - Hierarchical Manager-Worker System: 0.10\n",
      "  Successful Tasks for Second - Hierarchical Manager-Worker System:\n",
      "    Easy: 1/5 successful\n",
      "    Medium: 0/3 successful\n",
      "    Hard: 0/2 successful\n",
      "\n",
      "--- Processing Run: Third ---\n",
      "  Overall Average for Third - Baseline System: 0.20\n",
      "  Successful Tasks for Third - Baseline System:\n",
      "    Easy: 1/5 successful\n",
      "    Medium: 0/3 successful\n",
      "    Hard: 1/2 successful\n",
      "  Overall Average for Third - Hierarchical Manager-Worker System: 0.30\n",
      "  Successful Tasks for Third - Hierarchical Manager-Worker System:\n",
      "    Easy: 3/5 successful\n",
      "    Medium: 0/3 successful\n",
      "    Hard: 0/2 successful\n"
     ]
    }
   ],
   "source": [
    "## Set Up Paths and Calculate All Averages and Success Counts\n",
    "\n",
    "# Get the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "\n",
    "# Define the common base path up to 'results_chrome_thesis'\n",
    "common_base_path_root = os.path.join(\n",
    "    current_working_directory,\n",
    "    'results_chrome_thesis'\n",
    ")\n",
    "\n",
    "# Define the different \"runs\" \n",
    "runs = ['first', 'second', 'third']\n",
    "\n",
    "# Define the different \"systems\" \n",
    "systems = {\n",
    "    'Baseline System': os.path.join('pyautogui', 'screenshot', 'ui-tars', 'chrome'),\n",
    "    'Hierarchical Manager-Worker System': os.path.join('pyautogui', 'screenshot', 'ui-tars-agent', 'chrome')\n",
    "}\n",
    "\n",
    "# Dictionary to store all results (average and success counts)\n",
    "# Structure: {run_name: {system_name: {'average': avg_value, 'success_counts': {'Easy': X, 'Medium': Y, 'Hard': Z}}}}\n",
    "all_processed_data = {}\n",
    "\n",
    "print(\"--- Calculating Averages and Analyzing Task Success Across All Runs and Systems ---\")\n",
    "\n",
    "for run in runs:\n",
    "    print(f\"\\n--- Processing Run: {run.capitalize()} ---\")\n",
    "    all_processed_data[run] = {} # Initialize dictionary for the current run\n",
    "\n",
    "    for system_name, system_relative_path in systems.items():\n",
    "        # Construct the full path for the current run and system\n",
    "        full_path = os.path.join(common_base_path_root, run, system_relative_path)\n",
    "        \n",
    "        category_name = f\"{run.capitalize()} - {system_name}\"\n",
    "                \n",
    "        # Calculate overall average\n",
    "        average = calculate_average_results(full_path, category_name)\n",
    "        \n",
    "        # Analyze task success by difficulty\n",
    "        success_counts = analyze_task_success(full_path, ALL_TASK_DIFFICULTIES)\n",
    "        \n",
    "        all_processed_data[run][system_name] = {\n",
    "            'average': average,\n",
    "            'success_counts': success_counts\n",
    "        }\n",
    "\n",
    "        if average is not None:\n",
    "            print(f\"  Overall Average for {category_name}: {average:.2f}\")\n",
    "        else:\n",
    "            print(f\"  Overall Average for {category_name}: No Data\")\n",
    "        \n",
    "        print(f\"  Successful Tasks for {category_name}:\")\n",
    "        for difficulty, count in success_counts.items():\n",
    "            total_tasks_in_difficulty = len(ALL_TASK_DIFFICULTIES[difficulty])\n",
    "            print(f\"    {difficulty}: {count}/{total_tasks_in_difficulty} successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "091dd6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Overall System Performance Comparison (Aggregated Across All Runs) ---\n",
      "\n",
      "System: Baseline System\n",
      "  Overall Average Score (from 3 of 3 runs with data): 0.17\n",
      "  Total Successful Tasks (across 3 runs):\n",
      "    Easy: 3/15 successful (20.0%)\n",
      "    Medium: 0/9 successful (0.0%)\n",
      "    Hard: 2/6 successful (33.3%)\n",
      "\n",
      "System: Hierarchical Manager-Worker System\n",
      "  Overall Average Score (from 3 of 3 runs with data): 0.23\n",
      "  Total Successful Tasks (across 3 runs):\n",
      "    Easy: 7/15 successful (46.7%)\n",
      "    Medium: 0/9 successful (0.0%)\n",
      "    Hard: 0/6 successful (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# --- Overall System Performance Comparison (Aggregated Across All Runs) ---\n",
    "\n",
    "# Initialize a dictionary to store aggregated data for each system\n",
    "system_overall_performance = {}\n",
    "\n",
    "for system_name in systems.keys():\n",
    "    system_overall_performance[system_name] = {\n",
    "        'run_averages': [],  # Stores average scores from each run\n",
    "        'total_success_counts': {}, # Stores summed success counts per difficulty\n",
    "        'runs_with_data_count': 0 # Counts runs contributing to the average score\n",
    "    }\n",
    "    # Initialize success counts for all known difficulties\n",
    "    for difficulty_level in ALL_TASK_DIFFICULTIES.keys():\n",
    "        system_overall_performance[system_name]['total_success_counts'][difficulty_level] = 0\n",
    "\n",
    "# Aggregate data from all_processed_data\n",
    "for run_name, systems_data_in_run in all_processed_data.items():\n",
    "    for system_name, data_for_system in systems_data_in_run.items():\n",
    "        if system_name in system_overall_performance: # Process only configured systems\n",
    "            # Aggregate average scores\n",
    "            if data_for_system['average'] is not None:\n",
    "                system_overall_performance[system_name]['run_averages'].append(data_for_system['average'])\n",
    "                system_overall_performance[system_name]['runs_with_data_count'] +=1\n",
    "            \n",
    "            # Aggregate success counts by difficulty\n",
    "            for difficulty, count in data_for_system['success_counts'].items():\n",
    "                if difficulty in system_overall_performance[system_name]['total_success_counts']:\n",
    "                    system_overall_performance[system_name]['total_success_counts'][difficulty] += count\n",
    "                else:\n",
    "                    # Handle unexpected difficulty levels if necessary\n",
    "                    system_overall_performance[system_name]['total_success_counts'][difficulty] = count\n",
    "\n",
    "# Print the overall system comparison\n",
    "print(\"\\n\\n--- Overall System Performance Comparison (Aggregated Across All Runs) ---\")\n",
    "\n",
    "num_total_runs = len(runs)\n",
    "\n",
    "for system_name, performance_data in system_overall_performance.items():\n",
    "    print(f\"\\nSystem: {system_name}\")\n",
    "    \n",
    "    # Calculate and print overall average score\n",
    "    if performance_data['run_averages']:\n",
    "        # Compute average from per-run averages\n",
    "        overall_system_avg = sum(performance_data['run_averages']) / len(performance_data['run_averages'])\n",
    "        print(f\"  Overall Average Score (from {len(performance_data['run_averages'])} of {num_total_runs} runs with data): {overall_system_avg:.2f}\")\n",
    "    else:\n",
    "        print(f\"  Overall Average Score: No Data available from any run.\")\n",
    "        \n",
    "    print(f\"  Total Successful Tasks (across {num_total_runs} runs):\")\n",
    "    if not ALL_TASK_DIFFICULTIES:\n",
    "        print(\"    Cannot display task success: ALL_TASK_DIFFICULTIES is undefined or empty.\")\n",
    "    else:\n",
    "        for difficulty, total_success_count in performance_data['total_success_counts'].items():\n",
    "            if difficulty in ALL_TASK_DIFFICULTIES:\n",
    "                # Total tasks for this difficulty = (tasks per run) * (number of runs)\n",
    "                tasks_per_run_for_difficulty = len(ALL_TASK_DIFFICULTIES[difficulty])\n",
    "                grand_total_tasks_for_difficulty = tasks_per_run_for_difficulty * num_total_runs\n",
    "                \n",
    "                if grand_total_tasks_for_difficulty > 0:\n",
    "                    success_rate = (total_success_count / grand_total_tasks_for_difficulty) * 100\n",
    "                    print(f\"    {difficulty}: {total_success_count}/{grand_total_tasks_for_difficulty} successful ({success_rate:.1f}%)\")\n",
    "                elif tasks_per_run_for_difficulty == 0:\n",
    "                     print(f\"    {difficulty}: {total_success_count}/0 successful (No tasks defined for this difficulty)\")\n",
    "                else:\n",
    "                     print(f\"    {difficulty}: {total_success_count} successful (Total possible tasks calculation error)\")\n",
    "            else:\n",
    "                # Fallback for difficulties not in the primary definition\n",
    "                print(f\"    {difficulty}: {total_success_count} successful (Warning: Difficulty not in ALL_TASK_DIFFICULTIES)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e02b8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
